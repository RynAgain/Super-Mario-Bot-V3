# Training Configuration for Super Mario Bros AI
# Hyperparameters for Dueling DQN training

training:
  # Core Learning Parameters
  learning_rate: 0.00025          # Adam optimizer learning rate
  batch_size: 32                  # Training batch size
  replay_buffer_size: 10000       # Experience replay buffer capacity (reduced for 8GB GPU)
  target_update_frequency: 1000   # Target network update interval (steps)
  
  # Exploration Strategy (Epsilon-Greedy)
  epsilon_start: 1.0              # Initial exploration rate
  epsilon_end: 0.01               # Final exploration rate
  epsilon_decay: 0.9995           # Decay rate per step (slower exploration decay)
  epsilon_decay_type: "exponential" # "exponential" or "linear"
  
  # Training Schedule
  max_episodes: 50000             # Maximum training episodes
  max_steps_per_episode: 18000    # Max steps per episode (5 min at 60 FPS)
  warmup_episodes: 1000           # Episodes before training starts
  save_frequency: 1000            # Model checkpoint save interval
  evaluation_frequency: 500       # Evaluation run interval
  
  # Visualization Settings
  enable_plotting: false          # Disable real-time plotting to prevent freezing
  
  # Optimization Settings
  optimizer: "Adam"               # Optimizer type
  loss_function: "Huber"          # Loss function ("Huber", "MSE")
  gradient_clipping: 10.0         # Gradient clipping threshold
  gamma: 0.99                     # Discount factor
  
  # Advanced Training Options
  double_dqn: true                # Enable Double DQN
  prioritized_replay: false       # Enable prioritized experience replay
  noisy_networks: false           # Enable noisy networks for exploration
  
  # Curriculum Learning
  curriculum:
    enabled: true
    phases:
      - name: "exploration"
        episodes: 10000
        epsilon_override: 0.8
        reward_scaling: 1.2
      - name: "optimization" 
        episodes: 30000
        epsilon_override: null
        reward_scaling: 1.0
      - name: "mastery"
        episodes: 10000
        epsilon_override: 0.05
        reward_scaling: 0.9

# Performance and Resource Management
performance:
  # GPU Settings
  device: "cuda"                  # "cuda", "cpu", or "auto"
  mixed_precision: true           # Enable automatic mixed precision
  compile_model: true             # Use torch.compile for optimization
  
  # Memory Management
  pin_memory: true                # Pin memory for faster GPU transfer
  num_workers: 4                  # DataLoader worker processes
  prefetch_factor: 2              # Prefetch batches per worker
  
  # Monitoring
  profile_training: false         # Enable PyTorch profiler
  log_gpu_memory: true           # Log GPU memory usage
  
# Checkpointing and Recovery
checkpointing:
  auto_save: true                 # Automatic checkpoint saving
  keep_best_n: 5                 # Keep N best performing checkpoints
  save_optimizer_state: true     # Save optimizer state in checkpoints
  compression: true               # Compress checkpoint files

# Network Configuration (referenced by validation)
network:
  # Frame Processing
  frame_stack_size: 4              # Number of frames to stack (optimized for performance)
  frame_size: [84, 84]            # Frame dimensions (height, width)
  
  # State Vector Configuration
  state_vector_size: 12           # Game state vector size (12 or 20)
  enhanced_features: false        # Enable enhanced 20-feature mode
  
  # Action Space
  num_actions: 12                 # Number of possible actions
  
  # Architecture
  conv_layers:
    - filters: 32
      kernel_size: 8
      stride: 4
      padding: 2
    - filters: 64
      kernel_size: 4
      stride: 2
      padding: 1
    - filters: 64
      kernel_size: 3
      stride: 1
      padding: 1
  
  # Dueling Architecture
  fusion_hidden_size: 512
  value_hidden_size: 256
  advantage_hidden_size: 256
  dropout_rate: 0.3

# Enhanced Memory Features Configuration
enhanced_memory:
  # Feature toggles (matches Lua script configuration)
  enabled: false                   # Master switch for enhanced features
  enemy_detection: true           # Enable enemy detection and tracking
  powerup_detection: true         # Enable power-up detection
  tile_sampling: true             # Enable level tile sampling around Mario
  enhanced_death_detection: true  # Enable enhanced death detection
  velocity_tracking: true         # Enable Mario velocity tracking
  
  # Binary payload configuration
  payload_size: 128               # Expected payload size from Lua script
  validate_payload: true          # Validate payload structure
  
  # Normalization parameters for enhanced features
  normalization:
    enemy_distance_max: 500.0     # Maximum enemy distance for normalization
    powerup_distance_max: 300.0   # Maximum power-up distance
    tile_value_max: 255.0         # Maximum tile value
    velocity_magnitude_max: 180.0  # Maximum velocity magnitude
    
  # Feature weights for reward calculation (future use)
  feature_weights:
    enemy_avoidance: 1.0          # Weight for enemy avoidance reward
    powerup_collection: 2.0       # Weight for power-up collection reward
    environmental_awareness: 0.5   # Weight for environmental features

# Enhanced Reward System Configuration
enhanced_rewards:
  # Master switch for enhanced rewards
  enabled: false                   # Enable enhanced reward calculation
  
  # Reward values for enhanced features
  powerup_collection_reward: 50.0  # Reward for collecting power-ups
  enemy_elimination_reward: 25.0   # Reward per enemy eliminated
  environmental_navigation_reward: 5.0  # Base reward for environmental navigation
  velocity_movement_multiplier: 0.1     # Multiplier for velocity-based rewards
  strategic_positioning_reward: 2.0     # Reward for safe positioning
  
  # Environmental navigation settings
  pit_avoidance_reward: 10.0       # Reward for avoiding pits
  obstacle_navigation_reward: 5.0  # Reward for navigating obstacles
  forward_momentum_threshold: 10.0 # Minimum velocity for momentum reward
  safe_distance_threshold: 100.0   # Safe distance from enemies (pixels)
  
  # Enhanced death penalties
  pit_death_penalty: -100.0        # Penalty for falling into pits
  enemy_collision_penalty: -50.0   # Penalty for enemy collisions
  time_death_penalty: -25.0        # Penalty for timeout deaths
  general_death_penalty: -10.0     # General death penalty
  
  # Feature weights for enhanced rewards
  feature_weights:
    powerup_collection: 1.0        # Weight for power-up collection rewards
    enemy_elimination: 1.0         # Weight for enemy elimination rewards
    environmental_awareness: 1.0   # Weight for environmental navigation
    velocity_movement: 1.0         # Weight for velocity-based rewards
    strategic_positioning: 1.0     # Weight for strategic positioning
  
  # Logging and analysis
  log_reward_components: true      # Log individual reward components
  reward_analysis_frequency: 100   # Analyze rewards every N episodes